# -*- coding: utf-8 -*-
"""Credit_Card_Fraud_Test (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Jx0R96qc_hnI4RJ3u-LQIOQW2PNWvMN

## **Identifying Fraudulent Transactions:**

**Objective of the Project:**

The objective of this project is to build a predictive model that classifies whether a given transaction is fraudulent or legitimate.

Detecting fraudulent transactions is critical in the financial and banking sectors to minimize monetary losses, protect customers, and maintain trust.

**Business Goal:**

Maximize fraud detection accuracy (high recall) while maintaining a low false positive rate (balanced precision-recall trade-off).

I imported the jsonlines library because the data is stored in Json Lines format, where each line was a separate JSON object.
This library helps read and process the data easily and efficiently, especially when working with large files.
"""

pip install jsonlines

import jsonlines
import pandas as pd
from datetime import datetime
#import sys
#print(sys.executable)

data = []

with jsonlines.open("/content/transactions data.txt") as reader:
  for obj in reader.iter(skip_invalid=True): # Used Skip_invalid as True to ignore any error related to extra characters after valid JSON Object
    data.append(obj)

data = pd.DataFrame(data)

"""## **Data Over View and Cleaning**:"""

print(data.info())

# Count unique values in each column
unique_counts = data.nunique()

# Display the result
print("Unique value count per column:")
print(unique_counts)

"""#### **Missing Values and Empty Strings**:
The dataset was first checked for missing values using the isnull().sum() method.
Additionally, some columns contained empty strings or whitespace characters instead of actual null values. Regular expressions (Regex) were used to detect and handle such columns appropriately.
"""

# Check remaining missing values
missing_summary = data.isnull().sum()
print(missing_summary)

import numpy as np

empty_counts_sum = data.replace(r'^\s*$', np.nan, regex=True).isna().sum()
print(empty_counts_sum[empty_counts_sum > 0])

"""**Some columns ‚Äî like echoBuffer, merchantCity, merchantState, merchantZip, posOnPremises, and recurringAuthInd ‚Äî contained only empty or missing values.
Since they don‚Äôt provide any useful information for fraud detection and would only increase processing time and memory usage, I decided to remove them from the dataset.**
"""

data = data.drop(columns=['customerId'])

cols_to_drop = [
    'echoBuffer', 'merchantCity', 'merchantState',
    'merchantZip', 'posOnPremises', 'posConditionCode',  'recurringAuthInd'
]
data = data.drop(columns=cols_to_drop)

import pandas as pd

date_cols = ["dateOfLastAddressChange", "accountOpenDate", "currentExpDate", "transactionDateTime"]

# Convert columns to datetime
for col in date_cols:
    data[col] = pd.to_datetime(data[col], errors="coerce")  # coerce will set invalid parsing to NaT

# Check result
print(data[date_cols].dtypes)

"""### **Exploratory Data Analysis:**

**Checking Skewness and Outliers**:
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

num_cols = ['creditLimit', 'availableMoney', 'transactionAmount', 'currentBalance']

# Calculate skewness
skew_vals = data[num_cols].skew()
print(skew_vals)

# Plot histograms + KDE for visualization
for col in num_cols:
    plt.figure(figsize=(6,4))
    sns.histplot(data[col], kde=True, bins=50)
    plt.title(f"{col} | Skewness = {data[col].skew():.2f}")
    plt.show()

"""The variables - Current Amount, Transaction Amount, Available Money are rightly skewed and  applying log1p transformation will help us in removing skewness and make data ditribution normal.

**Analyzing the Impact of CVV Mismatch on Fraudulent Transactions**
"""

# Create new column checking equality
data['cvv_match'] = data['cardCVV'] == data['enteredCVV']
data['cvv_match'].value_counts()

# Filter mismatches
mismatches = data.loc[~data['cvv_match']]

# Group by merchant and count mismatches
merchant_mismatch_counts = mismatches.groupby('merchantCategoryCode').size().reset_index(name='mismatch_count').sort_values(by='mismatch_count', ascending=False)

print(merchant_mismatch_counts)

"""The merchant category analysis revealed significant data mismatches, particularly in Online Retail (152), Fast Food (90), and Food (61). Such inconsistencies may be indicative of fraudulent behavior, as high-frequency or online transactions are often used by fraudsters to test stolen cards.

These mismatches could arise from card-not-present activities. These categories should be prioritized for anomaly detection or model-based fraud risk scoring.

Conversely, categories like Airline and Fuel show lower mismatch counts, suggesting greater consistency and lower immediate fraud risk.
"""

data = data.drop(columns=['cardCVV', 'enteredCVV', 'cardLast4Digits'])

"""### **Analyzing Fraud Rate by POS Entry Mode**"""

counts = data['posEntryMode'].value_counts()
counts
data.groupby('posEntryMode')['isFraud'].mean().sort_values(ascending=False)

import matplotlib.pyplot as plt

fraud_rates = {
    "09": 0.021909,
    "02": 0.016152,
    "80": 0.015385,
    "90": 0.010397,
    "05": 0.007058
}

plt.figure(figsize=(8,5))
plt.bar(fraud_rates.keys(), [v*100 for v in fraud_rates.values()], color="tomato")
plt.axhline(5.5749, color="black", linestyle="--", label="Overall Fraud Rate (5.57%)")

plt.xlabel("posEntryMode")
plt.ylabel("Fraud Rate (%)")
plt.title("Fraud Rate by POS Entry Mode")
plt.legend()
plt.show()

data["credit_utilization"] = data["transactionAmount"] / (data["availableMoney"] + 1)

data["country_mismatch"] = (data["acqCountry"] != data["merchantCountryCode"]).astype(int)

"""*Merchant Level Risk Features*"""

data["merchant_total_txns"] = data.groupby("merchantName")["transactionAmount"].transform("count")

data["merchant_avg_amt"] = data.groupby("merchantName")["transactionAmount"].transform("mean")

data["merchant_fraud_rate"] = data.groupby("merchantName")["isFraud"].transform("mean")

data["avg_txn_amt_card"] = data.groupby("accountNumber")["transactionAmount"].transform("mean")
data["std_txn_amt_card"] = data.groupby("accountNumber")["transactionAmount"].transform("std")
data["amt_deviation"] = (data["transactionAmount"] - data["avg_txn_amt_card"]) / (data["std_txn_amt_card"] + 1)

"""The fraud rate varies by POS entry mode, with mode 09 showing the highest fraud rate (~2.2%), while mode 05 exhibits the lowest.
However, all modes have a fraud rate below the overall average of 5.57%, suggesting that POS-based fraud is relatively contained compared to other channels.
This may indicate stronger fraud controls or verification mechanisms within physical or chip-based POS transactions.‚Äù

**Lets identify ranges where fraud rate increases for Creditlimit, transactionAmount and currentBalance.**
"""

# Define your columns of interest
cols_to_check = ['creditLimit', 'transactionAmount', 'currentBalance']

# Set the step size for thresholds (adjust for granularity)
threshold_steps = 20  # percentiles from 10% to 90%

# Loop over each column
for col in cols_to_check:
    print(f"\nüìä Fraud Rate Analysis for: {col}")
    print("-" * 40)

    # Get min and max thresholds using percentiles
    for percentile in range(10, 100, threshold_steps):
        threshold = data[col].quantile(percentile / 100)

        fraud_rate_above = data[data[col] > threshold]['isFraud'].mean()
        fraud_rate_below = data[data[col] <= threshold]['isFraud'].mean()

        print(f"Threshold > {percentile}% ({threshold:.2f}) ‚Üí Fraud Rate: {fraud_rate_above:.4f}")

"""Fraud rate remains relatively stable (~1.5‚Äì1.7%) until very high credit limits  20,000.

A sharp rise at 90% suggests that very high credit limits are associated with elevated fraud risk ‚Äî possibly due to opportunistic misuse.

**Insight:** Monitor accounts with exceptionally high credit limits, as they show twice the average fraud rate.

Fraud rate steadily increases with transaction size.
With $329, fraud rate more than doubles from the base level.

**Insight:** Larger transactions are consistently riskier ‚Äî a clear positive relationship between amount and fraud likelihood.

Fraud rates rise with higher transaction amounts and credit limits, showing that larger transactions and high-limit accounts are more prone to fraud.

### **TOP 10 MERCHANTS BY THE FRAUD RATE:**
"""

data['fraud_amount'] = data['transactionAmount'] * data['isFraud']

merchant_fraud = (
    data.groupby('merchantName', as_index=False)
      .agg(
          total_txns=('isFraud', 'count'),
          fraud_txns=('isFraud', 'sum'),
          fraud_rate=('isFraud', 'mean'),
          fraud_amount=('fraud_amount', 'sum')
      )
      .sort_values(by='fraud_rate', ascending=False)
)

merchant_fraud.head(8)

"""Analysed merchant level patterns, American Airlines Showed high fraud rate and followed by IN AND OUT,  indicating vulnerablities at specific outlets.

### **TOP 10 MERCHANTS BY THE FRAUD TRANSACTIONS COUNT AND FRAUD AMOUNT:**
"""

top_merchants = (
    data.assign(fraud_amount=data['transactionAmount'] * data['isFraud'])
        .groupby('merchantName', as_index=False)
        .agg(
            total_txns=('isFraud', 'size'),
            fraud_txns=('isFraud', 'sum'),
            total_amount=('transactionAmount', 'sum'),
            fraud_amount=('fraud_amount', 'sum')
        )
        .assign(fraud_rate=lambda d: d['fraud_txns'] / d['total_txns'])
        .nlargest(10, 'fraud_txns')[['merchantName', 'fraud_rate', 'fraud_txns', 'fraud_amount']]
)

print(top_merchants)

"""‚Äú**The top merchants by fraud volume include Lyft, eBay, and Fresh Flowers.
Lyft has the highest number of fraudulent transactions (760), though Fresh Flowers shows the highest fraud rate (6.6%).
Fraud losses are concentrated among large, high-volume merchants, with several well-known e-commerce platforms (eBay, Walmart, Alibaba) showing notable exposure.**

Fraud risk isn‚Äôt just about frequency ‚Äî it‚Äôs a combination of rate, transaction volume, and dollar loss.

### **Merchants list by hourly and Temporal Patterns**
"""

import numpy as np

TOP_K = 3  # adjust as needed

# Aggregate once, with vectorized fraud_amount
agg = (
    data.assign(
        isFraud=lambda d: d['isFraud'].astype('i1'),
        fraud_amount=lambda d: d['transactionAmount'] * d['isFraud'],
    )
    .groupby(['Trans_Hour', 'merchantName'], as_index=False)
    .agg(
        total_txns=('isFraud', 'size'),
        fraud_txns=('isFraud', 'sum'),
        total_amount=('transactionAmount', 'sum'),
        fraud_amount=('fraud_amount', 'sum'),
    )
)

# % fraud amount out of total amount (safe divide)
agg['pct_fraud_amount'] = np.divide(
    agg['fraud_amount'],
    agg['total_amount'],
    out=np.zeros(len(agg), dtype=float),
    where=agg['total_amount'].ne(0)
)

# Top-K merchants per hour by fraud_amount, efficiently
top_idx = (
    agg.groupby('Trans_Hour')['fraud_amount']
       .nlargest(TOP_K)
       .index.get_level_values(1)
)

top_merchants_per_hour = (
    agg.loc[top_idx]
       .sort_values(['Trans_Hour', 'fraud_amount'], ascending=[True, False])
)

# Example: preview
top_merchants_per_hour.head(20)

"""### **FEATURE ENGINEERING**

**Generating New Features from Date and Time Columns**

---
"""

data['Trans_Hour'] = data['transactionDateTime'].dt.hour
data["is_night"] = data["Trans_Hour"].isin([0,1,2,3,4,5]).astype(int)

# Create new feature "validity" in days (or years)
data['validity_days'] = (data['currentExpDate'] - data['accountOpenDate']).dt.days
data['validity_years'] = data['validity_days'] / 365

# Drop the original columns
df = data.drop(['currentExpDate', 'accountOpenDate','expirationDateKeyInMatch'], axis=1)

"""Extracted the transaction hour from the transactionDateTime column to analyze time-based patterns in fraud (e.g., late-night transactions).

Created a binary feature is_night to flag transactions occurring between 12 AM and 6 AM, since fraudulent activities are often higher during odd hours.

Generated a new feature validity_days (and validity_years) by calculating the difference between account opening and expiry date ‚Äî this helps understand how long an account has been active, which can influence fraud risk.

Dropped the original date columns (currentExpDate, accountOpenDate, etc.) after deriving useful features from them to reduce redundancy.

Extracted transaction date, year, and month name to capture potential seasonal or monthly trends in fraudulent transactions

**Reducing Cardinality in Merchant Names**
"""

# pick top N merchants (say top 20 by frequency)
topN = 20
top_merchants = df['merchantName'].value_counts().nlargest(topN).index

# replace rare ones with 'Other'
df['merchantName_top'] = df['merchantName'].where(df['merchantName'].isin(top_merchants), 'Other')

# confirm
print(df['merchantName_top'].value_counts())

# Drop the original column
df.drop(columns=['merchantName'], inplace=True)

"""I checked the frequency of each merchant and grouped those with fewer than 200 transactions under ‚ÄòOther‚Äô. This reduced high cardinality, improved model efficiency, and prevented overfitting. It also helped the model focus on merchants with enough data to learn meaningful fraud patterns

simplifying categorical encoding, making the dataset cleaner and easier to interpret.


"""

df = df.drop(['dateOfLastAddressChange', 'transactionDateTime'], axis=1)
df

print(df.columns)

import matplotlib.pyplot as plt

# Plot histogram for transactionAmount
plt.figure(figsize=(10, 6))
plt.hist(df["transactionAmount"], bins=50, color='blue', edgecolor='black', alpha=0.7)
plt.title("Histogram of Transaction Amounts")
plt.xlabel("Transaction Amount")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

# Statistical description of transactionAmount
transaction_stats = df["transactionAmount"].describe()
print("Transaction Amount Summary:")
print(transaction_stats)

"""The histogram above illustrates the distribution of transaction amounts and their frequency. The data shows that transaction peaks occur predominantly for amounts less than **250,** with a mean transaction value of **136.** Additionally, **75%** of users spend **191** or less on their transactions, indicating that the majority of users make transactions below **200**. If later we see any spends above then this 75% it may indicate fraudulent transaction.

### **Data Transformation & Pipeline Setup**
"""

from imblearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
import numpy as np

# Remove accountNumber & merchantName to avoid leakage and explosion
feature_order = (
    ["creditLimit","availableMoney","amt_deviation","validity_years","validity_days",
     "Trans_Hour","merchant_avg_amt","credit_utilization","avg_txn_amt_card","std_txn_amt_card"] + ["amt_deviation"] +
    ["transactionAmount","currentBalance","merchant_total_txns"] +
    ["acqCountry","merchantCountryCode","posEntryMode","merchantCategoryCode","transactionType","merchantName_top"] +
    ["cardPresent","cvv_match","country_mismatch","is_night"]
)

X = df[feature_order].copy()
y = df["isFraud"].copy()

categorical_cols = ["acqCountry","merchantCountryCode","posEntryMode",
                    "merchantCategoryCode","transactionType","merchantName_top"]

binary_cols = ["cardPresent","cvv_match","country_mismatch","is_night"]

num_cols = ["creditLimit","availableMoney","validity_years","validity_days","Trans_Hour",
            "merchant_avg_amt","credit_utilization","avg_txn_amt_card","std_txn_amt_card"]

log_num_cols = ["transactionAmount","currentBalance","merchant_total_txns"]

passthrough_cols = ["amt_deviation"]

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    # keep sparse to avoid densifying huge matrices
    ("onehot", OneHotEncoder(handle_unknown="ignore", drop="first", sparse_output=True))
])
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])
log_numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("log", FunctionTransformer(np.log1p, validate=False)),
    ("scaler", StandardScaler())
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, num_cols),
        ("log_num", log_numeric_transformer, log_num_cols),
        ("cat", categorical_transformer, categorical_cols),
        ("bin", "passthrough", binary_cols),
    ],
    sparse_threshold=1.0  # keep sparse if any transformer returns sparse
)

pipe = Pipeline(steps=[
    ("preprocessor", preprocessor),
    # no SMOTE; handle imbalance in the model
    ("model", LogisticRegression(max_iter=1000, class_weight="balanced", solver="saga"))
])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)

y_pred_proba = pipe.predict_proba(X_test)[:,1]
y_pred_proba

print(y_train)

import pandas as pd

# Create a comparison DataFrame
comparison = pd.DataFrame({
    "Actual": y_test.values,
    "Predicted": y_pred,
    "Predicted_Prob": y_pred_proba
})

# Show first few rows
print(comparison.head(30))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

print("Original X shape:", X.shape)
print("After split:", X_train.shape, X_test.shape)

# Original dataset
print("Original X shape:", X.shape)
print("After split:", X_train.shape, X_test.shape)

# Count total frauds before and after split
print("\nFraud counts:")
print(f"Total frauds in full dataset: {sum(y)}")
print(f"Frauds in training set: {sum(y_train)}")
print(f"Frauds in test set: {sum(y_test)}")

# Optional: also print fraud ratios (percentage of frauds)
print("\nFraud ratios:")
print(f"Overall fraud ratio: {y.mean():.4%}")
print(f"Train fraud ratio: {y_train.mean():.4%}")
print(f"Test fraud ratio: {y_test.mean():.4%}")

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score

confusion_matrix(y_test,y_pred)

roc_auc_score(y_test, y_pred_proba)

threshold = 0.63
y_pred_proba2 = pipe.predict_proba(X_test)[:,1]
y_pred2 = (y_pred_proba >= threshold).astype(int)

from sklearn.metrics import roc_auc_score
roc_auc_score(y_test, y_pred_proba2)

classification_report_output_2 = classification_report(y_test, y_pred2)
print(classification_report_output_2)
accuracy_score(y_test,y_pred2)

confusion_matrix(y_test,y_pred2)

import pandas as pd

# Create a comparison DataFrame
comparison = pd.DataFrame({
    "Actual": y_test.values,
    "Predicted": y_pred2,
    "Predicted_Prob": y_pred_proba2
})

# Show first few rows
print(comparison.head(30))

"""Data is fitting so well on training data, and not able to generalize well on test data, high bias and low variance. We can use xgboost to lower the bias.

### **Modeling & Hyperparameter Tuning**

**XGBOOST:**
"""

from sklearn.pipeline import Pipeline   # Corrected alias
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix

# XGBoost pipeline
xgb_pipe = Pipeline(steps=[
    ("preprocessor", preprocessor),  # reuse your existing preprocessor
    ("model", XGBClassifier(
        n_estimators=700,
        max_depth=5,
        learning_rate=0.04,
        subsample=0.6,
        colsample_bytree=0.6,
        random_state=42,
        scale_pos_weight=(
            len(y_train[y_train == 0]) / len(y_train[y_train == 1])
        ),
        n_jobs=-1,
        use_label_encoder=False,
        eval_metric="logloss"  # avoids warnings
    ))
])

# Train model
xgb_pipe.fit(X_train, y_train)

# Predictions
xgb_pred = xgb_pipe.predict(X_test)
xgb_proba = xgb_pipe.predict_proba(X_test)[:, 1]

# Evaluation
print("XGBoost Classifier")
print(classification_report(y_test, xgb_pred))
print("ROC AUC:", roc_auc_score(y_test, xgb_proba))
print("Confusion Matrix:\n", confusion_matrix(y_test, xgb_pred))

import numpy as np

threshold = 0.60

xgb_proba_2 = xgb_pipe.predict_proba(X_test)[:, 1]
xgb_pred_2 = (xgb_proba_2 >= threshold).astype(int)

print(classification_report(y_test, xgb_pred_2))

import pandas as pd

# Create the comparison DataFrame
comparison = pd.DataFrame({
    "Actual": y_test.values,
    "Predicted": xgb_pred_2,
    "Predicted_Prob": xgb_proba_2
})

# Filter only rows where Actual == 1
true_only = comparison[comparison["Actual"] == 1]

# Sort by predicted probability (highest first)
true_only_sorted = true_only.sort_values(by="Predicted_Prob", ascending=False)

# Show the first 30 rows
print(true_only_sorted.head(50))

print("Confusion Matrix:\n", confusion_matrix(y_test, xgb_pred_2))

import pickle

with open('xgb_pipe.pkl', 'wb') as model_file:
    pickle.dump(xgb_pipe, model_file)

import pickle

with open('xgb_pipe.pkl', 'rb') as model_file:
 loaded_model = pickle.load(model_file)

import pickle

loaded_model.named_steps['preprocessor'].feature_names_in_

# Make predictions with the loaded model

# 1Ô∏è‚É£ Get model predictions and probabilities
loaded_xgb_pred_proba = loaded_model.predict_proba(X_test)[:, 1]

# 2Ô∏è‚É£ Apply custom threshold
threshold = 0.6
loaded_xgb_pred = (loaded_xgb_pred_proba >= threshold).astype(int)

# 3Ô∏è‚É£ Evaluate
print("Loaded XGBoost Classifier Evaluation (Threshold = 0.6)")
print(classification_report(y_test, loaded_xgb_pred))
print("ROC AUC:", roc_auc_score(y_test, loaded_xgb_pred_proba))
print("Confusion Matrix:\n", confusion_matrix(y_test, loaded_xgb_pred))

"""**Feature Importance**"""

def ct_feature_names(preprocessor: ColumnTransformer, input_cols):
    # 1) If supported, use the built-in method
    if hasattr(preprocessor, "get_feature_names_out"):
        try:
            return preprocessor.get_feature_names_out(input_features=input_cols)
        except Exception:
            pass

    # 2) Manual walk: handles OHE (via categories_) + passthrough + pipelines
    names = []
    used = set()

    for name, trans, cols in preprocessor.transformers_:
        if name == "remainder" or trans == "drop":
            continue

        # normalize cols
        if isinstance(cols, slice):
            cols = list(input_cols[cols])
        elif isinstance(cols, (np.ndarray, list, tuple)):
            cols = list(cols)
        elif isinstance(cols, str):
            cols = [cols]

        used.update(cols)

        # unwrap Pipeline
        if isinstance(trans, Pipeline):
            trans = trans.steps[-1][1]

        # OneHotEncoder: build names from categories_
        if isinstance(trans, OneHotEncoder) and hasattr(trans, "categories_"):
            for c, cats in zip(cols, trans.categories_):
                for cat in cats:
                    names.append(f"onehot__{c}_{cat}")
            continue

        # Transformers without expansion (scaler, imputer, etc.) ‚Üí one name per input
        names.extend(cols)

    # remainder passthrough
    if getattr(preprocessor, "remainder", None) == "passthrough":
        rem = [c for c in input_cols if c not in used]
        names.extend(rem)

    return np.array(names, dtype=object)

# ---- use it
pre = xgb_pipe.named_steps["preprocessor"]        # NOT the estimator
feature_names = ct_feature_names(pre, X_train.columns)
print(len(feature_names), feature_names[:10])

import pandas as pd

est = xgb_pipe.named_steps["model"]
booster = est.get_booster()
gain = booster.get_score(importance_type="gain")      # keys: f0, f1, ...
idx2name = {f"f{i}": n for i, n in enumerate(feature_names)}
feature_Importance_gain = (pd.Series({idx2name.get(k, k): v for k, v in gain.items()})
           .sort_values(ascending=False))
print(feature_Importance_gain.head(25))

feature_Importance_gain.to_csv("xgb_feature_importance_gain.csv")
print(feature_Importance_gain)

# Save your feature importance to CSV
feature_Importance_gain.to_csv("xgb_feature_importance_gain.csv", header=["gain_importance"])
print("‚úÖ File saved: xgb_feature_importance_gain.csv")



"""**TOP PREDICTORS OF FRAUD:**
**Merchants and Peak Hours to Watch out:**

Uber, Lyft, Ebay.com, Walmart, discount, Gap and Sears consistently appeared in list where Fraud transaction volume and fraud counts were high. This evidented from the temporal analysis where hours like 12:00 AM, 01:00 AM and 03:00 AM were targeted mostly and these specific merchants showed fraudulent activity indicating low monitoring hours or bot testing.

"""